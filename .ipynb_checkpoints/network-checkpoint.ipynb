{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "adjacent-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "speaking-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel (r'data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "active-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['Fallnummer'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "aggregate-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientClassificationNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, activation=torch.tanh):\n",
    "        super(PatientClassificationNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.activation = activation\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "progressive-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PatientClassificationNet(4,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "lasting-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:,['AGE','Admission type 2','FA ab para','No. of times sent to ICU']]\n",
    "X -= X.mean()\n",
    "X /= X.std()\n",
    "X = torch.Tensor(X.to_numpy())\n",
    "Y = df.loc[:,['Tod']]\n",
    "Y = torch.Tensor(Y.to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "satellite-research",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 4000\n",
    "X_train = X[:num_train]\n",
    "y_train = Y[:num_train]\n",
    "X_test = X[num_train:]\n",
    "y_test = Y[num_train:]\n",
    "dataset_train = TensorDataset(X_train, y_train)\n",
    "dataset_test = TensorDataset(X_test, y_test)\n",
    "trainloader = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "testloader = DataLoader(dataset_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "horizontal-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = y_train.nonzero().size(0)/num_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "blank-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, testloader, epoch=5):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "    for epoch in range(epoch):\n",
    "        total_loss = 0\n",
    "        net.train()\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "#             print(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss\n",
    "        \n",
    "        net.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            outputs = net(inputs)\n",
    "            correct += ((net(inputs)>threshold) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        acc_train = correct / total\n",
    "#         print(f'epoch {epoch}: accuracy: {acc} loss:{total_loss}')\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(testloader):\n",
    "            inputs, labels = data\n",
    "            outputs = net(inputs)\n",
    "            correct += ((net(inputs)>threshold) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        acc = correct / total\n",
    "        print(f'epoch {epoch}: accuracy1: {acc} accuracy2: {acc_train} loss:{total_loss}')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "muslim-football",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: accuracy1: 0.08642911296436695 accuracy2: 0.072 loss:61.952781677246094\n",
      "epoch 1: accuracy1: 0.08642911296436695 accuracy2: 0.072 loss:36.767417907714844\n",
      "epoch 2: accuracy1: 0.38817285822592873 accuracy2: 0.35325 loss:32.001277923583984\n",
      "epoch 3: accuracy1: 0.5223654283548143 accuracy2: 0.496 loss:30.851041793823242\n",
      "epoch 4: accuracy1: 0.5837755875663382 accuracy2: 0.55675 loss:30.418376922607422\n",
      "epoch 5: accuracy1: 0.6285064442759667 accuracy2: 0.58825 loss:30.180105209350586\n",
      "epoch 6: accuracy1: 0.640636846095527 accuracy2: 0.59825 loss:30.024890899658203\n",
      "epoch 7: accuracy1: 0.6611068991660348 accuracy2: 0.614 loss:29.917390823364258\n",
      "epoch 8: accuracy1: 0.6717210007581501 accuracy2: 0.621 loss:29.83405113220215\n",
      "epoch 9: accuracy1: 0.6884003032600455 accuracy2: 0.634 loss:29.782363891601562\n",
      "epoch 10: accuracy1: 0.689158453373768 accuracy2: 0.63675 loss:29.7412052154541\n",
      "epoch 11: accuracy1: 0.6914329037149356 accuracy2: 0.63925 loss:29.712575912475586\n",
      "epoch 12: accuracy1: 0.6929492039423806 accuracy2: 0.642 loss:29.692306518554688\n",
      "epoch 13: accuracy1: 0.690674753601213 accuracy2: 0.63975 loss:29.67576789855957\n",
      "epoch 14: accuracy1: 0.690674753601213 accuracy2: 0.64 loss:29.665658950805664\n",
      "epoch 15: accuracy1: 0.6929492039423806 accuracy2: 0.64125 loss:29.654897689819336\n",
      "epoch 16: accuracy1: 0.6929492039423806 accuracy2: 0.6425 loss:29.647342681884766\n",
      "epoch 17: accuracy1: 0.6974981046247157 accuracy2: 0.64675 loss:29.639118194580078\n",
      "epoch 18: accuracy1: 0.6974981046247157 accuracy2: 0.64625 loss:29.637495040893555\n",
      "epoch 19: accuracy1: 0.6974981046247157 accuracy2: 0.64625 loss:29.63176155090332\n",
      "epoch 20: accuracy1: 0.6967399545109931 accuracy2: 0.64575 loss:29.632112503051758\n",
      "epoch 21: accuracy1: 0.6974981046247157 accuracy2: 0.64625 loss:29.62299156188965\n",
      "epoch 22: accuracy1: 0.6921910538286581 accuracy2: 0.64175 loss:29.61910057067871\n",
      "epoch 23: accuracy1: 0.6921910538286581 accuracy2: 0.64175 loss:29.618980407714844\n",
      "epoch 24: accuracy1: 0.6982562547384382 accuracy2: 0.64725 loss:29.61863136291504\n",
      "epoch 25: accuracy1: 0.6990144048521607 accuracy2: 0.6475 loss:29.61371612548828\n",
      "epoch 26: accuracy1: 0.6982562547384382 accuracy2: 0.647 loss:29.606643676757812\n",
      "epoch 27: accuracy1: 0.6990144048521607 accuracy2: 0.6475 loss:29.605688095092773\n",
      "epoch 28: accuracy1: 0.7005307050796058 accuracy2: 0.65 loss:29.605487823486328\n",
      "epoch 29: accuracy1: 0.6990144048521607 accuracy2: 0.64725 loss:29.59798240661621\n",
      "epoch 30: accuracy1: 0.6974981046247157 accuracy2: 0.64725 loss:29.597412109375\n",
      "epoch 31: accuracy1: 0.6937073540561031 accuracy2: 0.64325 loss:29.593185424804688\n",
      "epoch 32: accuracy1: 0.6997725549658832 accuracy2: 0.6485 loss:29.589412689208984\n",
      "epoch 33: accuracy1: 0.7043214556482184 accuracy2: 0.65825 loss:29.585451126098633\n",
      "epoch 34: accuracy1: 0.6990144048521607 accuracy2: 0.6475 loss:29.58331298828125\n",
      "epoch 35: accuracy1: 0.6944655041698257 accuracy2: 0.6435 loss:29.583942413330078\n",
      "epoch 36: accuracy1: 0.6997725549658832 accuracy2: 0.6485 loss:29.58016586303711\n",
      "epoch 37: accuracy1: 0.6997725549658832 accuracy2: 0.6485 loss:29.5821533203125\n",
      "epoch 38: accuracy1: 0.6914329037149356 accuracy2: 0.64325 loss:29.567934036254883\n",
      "epoch 39: accuracy1: 0.6914329037149356 accuracy2: 0.6435 loss:29.575326919555664\n",
      "epoch 40: accuracy1: 0.6899166034874905 accuracy2: 0.642 loss:29.572025299072266\n",
      "epoch 41: accuracy1: 0.6899166034874905 accuracy2: 0.64225 loss:29.56966781616211\n",
      "epoch 42: accuracy1: 0.6899166034874905 accuracy2: 0.642 loss:29.565446853637695\n",
      "epoch 43: accuracy1: 0.6944655041698257 accuracy2: 0.64625 loss:29.563213348388672\n",
      "epoch 44: accuracy1: 0.6997725549658832 accuracy2: 0.65625 loss:29.562578201293945\n",
      "epoch 45: accuracy1: 0.690674753601213 accuracy2: 0.643 loss:29.56420135498047\n",
      "epoch 46: accuracy1: 0.6921910538286581 accuracy2: 0.644 loss:29.55933380126953\n",
      "epoch 47: accuracy1: 0.6944655041698257 accuracy2: 0.64625 loss:29.552534103393555\n",
      "epoch 48: accuracy1: 0.7005307050796058 accuracy2: 0.656 loss:29.55483627319336\n",
      "epoch 49: accuracy1: 0.7050796057619408 accuracy2: 0.6615 loss:29.547557830810547\n",
      "epoch 50: accuracy1: 0.7050796057619408 accuracy2: 0.66175 loss:29.548187255859375\n",
      "epoch 51: accuracy1: 0.7005307050796058 accuracy2: 0.65975 loss:29.550830841064453\n",
      "epoch 52: accuracy1: 0.6921910538286581 accuracy2: 0.64375 loss:29.546566009521484\n",
      "epoch 53: accuracy1: 0.6914329037149356 accuracy2: 0.645 loss:29.54299545288086\n",
      "epoch 54: accuracy1: 0.6990144048521607 accuracy2: 0.65775 loss:29.5377197265625\n",
      "epoch 55: accuracy1: 0.6997725549658832 accuracy2: 0.662 loss:29.53485107421875\n",
      "epoch 56: accuracy1: 0.7005307050796058 accuracy2: 0.66025 loss:29.537878036499023\n",
      "epoch 57: accuracy1: 0.6990144048521607 accuracy2: 0.65875 loss:29.533306121826172\n",
      "epoch 58: accuracy1: 0.6921910538286581 accuracy2: 0.644 loss:29.53414535522461\n",
      "epoch 59: accuracy1: 0.6921910538286581 accuracy2: 0.643 loss:29.531082153320312\n",
      "epoch 60: accuracy1: 0.6990144048521607 accuracy2: 0.6575 loss:29.528377532958984\n",
      "epoch 61: accuracy1: 0.6921910538286581 accuracy2: 0.643 loss:29.522727966308594\n",
      "epoch 62: accuracy1: 0.7005307050796058 accuracy2: 0.6605 loss:29.522090911865234\n",
      "epoch 63: accuracy1: 0.6997725549658832 accuracy2: 0.66075 loss:29.522388458251953\n",
      "epoch 64: accuracy1: 0.7005307050796058 accuracy2: 0.6605 loss:29.524078369140625\n",
      "epoch 65: accuracy1: 0.6959818043972706 accuracy2: 0.65425 loss:29.52096939086914\n",
      "epoch 66: accuracy1: 0.6990144048521607 accuracy2: 0.65825 loss:29.5158634185791\n",
      "epoch 67: accuracy1: 0.6997725549658832 accuracy2: 0.66175 loss:29.51116943359375\n",
      "epoch 68: accuracy1: 0.6990144048521607 accuracy2: 0.65775 loss:29.514921188354492\n",
      "epoch 69: accuracy1: 0.6967399545109931 accuracy2: 0.652 loss:29.51403045654297\n",
      "epoch 70: accuracy1: 0.6967399545109931 accuracy2: 0.652 loss:29.50873565673828\n",
      "epoch 71: accuracy1: 0.6967399545109931 accuracy2: 0.65175 loss:29.5042781829834\n",
      "epoch 72: accuracy1: 0.6959818043972706 accuracy2: 0.654 loss:29.503652572631836\n",
      "epoch 73: accuracy1: 0.6959818043972706 accuracy2: 0.653 loss:29.504602432250977\n",
      "epoch 74: accuracy1: 0.6959818043972706 accuracy2: 0.655 loss:29.501365661621094\n",
      "epoch 75: accuracy1: 0.6967399545109931 accuracy2: 0.65225 loss:29.499784469604492\n",
      "epoch 76: accuracy1: 0.6967399545109931 accuracy2: 0.65275 loss:29.49728775024414\n",
      "epoch 77: accuracy1: 0.6959818043972706 accuracy2: 0.6575 loss:29.496965408325195\n",
      "epoch 78: accuracy1: 0.6959818043972706 accuracy2: 0.65725 loss:29.493797302246094\n",
      "epoch 79: accuracy1: 0.6967399545109931 accuracy2: 0.65375 loss:29.49077606201172\n",
      "epoch 80: accuracy1: 0.6959818043972706 accuracy2: 0.65525 loss:29.49091339111328\n",
      "epoch 81: accuracy1: 0.6990144048521607 accuracy2: 0.6585 loss:29.489553451538086\n",
      "epoch 82: accuracy1: 0.6959818043972706 accuracy2: 0.6565 loss:29.489355087280273\n",
      "epoch 83: accuracy1: 0.6959818043972706 accuracy2: 0.65475 loss:29.479263305664062\n",
      "epoch 84: accuracy1: 0.6997725549658832 accuracy2: 0.661 loss:29.48538589477539\n",
      "epoch 85: accuracy1: 0.6959818043972706 accuracy2: 0.656 loss:29.485347747802734\n",
      "epoch 86: accuracy1: 0.6997725549658832 accuracy2: 0.66 loss:29.479446411132812\n",
      "epoch 87: accuracy1: 0.6944655041698257 accuracy2: 0.65425 loss:29.47842025756836\n",
      "epoch 88: accuracy1: 0.6944655041698257 accuracy2: 0.65625 loss:29.4819393157959\n",
      "epoch 89: accuracy1: 0.6997725549658832 accuracy2: 0.661 loss:29.47273826599121\n",
      "epoch 90: accuracy1: 0.6944655041698257 accuracy2: 0.65625 loss:29.47355842590332\n",
      "epoch 91: accuracy1: 0.6944655041698257 accuracy2: 0.6535 loss:29.470458984375\n",
      "epoch 92: accuracy1: 0.6929492039423806 accuracy2: 0.653 loss:29.472566604614258\n",
      "epoch 93: accuracy1: 0.6944655041698257 accuracy2: 0.65475 loss:29.468996047973633\n",
      "epoch 94: accuracy1: 0.6944655041698257 accuracy2: 0.65575 loss:29.471263885498047\n",
      "epoch 95: accuracy1: 0.6982562547384382 accuracy2: 0.659 loss:29.466604232788086\n",
      "epoch 96: accuracy1: 0.6944655041698257 accuracy2: 0.655 loss:29.46428680419922\n",
      "epoch 97: accuracy1: 0.6944655041698257 accuracy2: 0.65575 loss:29.465295791625977\n",
      "epoch 98: accuracy1: 0.6929492039423806 accuracy2: 0.654 loss:29.462146759033203\n",
      "epoch 99: accuracy1: 0.6944655041698257 accuracy2: 0.65575 loss:29.45806121826172\n"
     ]
    }
   ],
   "source": [
    "train(net, trainloader, testloader, epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "concerned-madrid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3397, -0.8964],\n",
       "        [ 0.8697, -0.8964],\n",
       "        [ 0.1138, -0.8964],\n",
       "        ...,\n",
       "        [-0.8941, -0.8964],\n",
       "        [ 0.7689,  1.1154],\n",
       "        [ 1.3233,  1.1154]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "united-mentor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1096], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X_train)[84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "naked-arrow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  12,    0],\n",
       "        [  73,    0],\n",
       "        [  80,    0],\n",
       "        [  84,    0],\n",
       "        [  89,    0],\n",
       "        [  94,    0],\n",
       "        [ 106,    0],\n",
       "        [ 123,    0],\n",
       "        [ 124,    0],\n",
       "        [ 126,    0],\n",
       "        [ 131,    0],\n",
       "        [ 188,    0],\n",
       "        [ 218,    0],\n",
       "        [ 260,    0],\n",
       "        [ 276,    0],\n",
       "        [ 287,    0],\n",
       "        [ 288,    0],\n",
       "        [ 307,    0],\n",
       "        [ 332,    0],\n",
       "        [ 344,    0],\n",
       "        [ 354,    0],\n",
       "        [ 358,    0],\n",
       "        [ 359,    0],\n",
       "        [ 369,    0],\n",
       "        [ 371,    0],\n",
       "        [ 378,    0],\n",
       "        [ 385,    0],\n",
       "        [ 391,    0],\n",
       "        [ 403,    0],\n",
       "        [ 421,    0],\n",
       "        [ 425,    0],\n",
       "        [ 432,    0],\n",
       "        [ 454,    0],\n",
       "        [ 455,    0],\n",
       "        [ 481,    0],\n",
       "        [ 515,    0],\n",
       "        [ 522,    0],\n",
       "        [ 538,    0],\n",
       "        [ 553,    0],\n",
       "        [ 566,    0],\n",
       "        [ 589,    0],\n",
       "        [ 596,    0],\n",
       "        [ 605,    0],\n",
       "        [ 610,    0],\n",
       "        [ 616,    0],\n",
       "        [ 623,    0],\n",
       "        [ 627,    0],\n",
       "        [ 636,    0],\n",
       "        [ 640,    0],\n",
       "        [ 646,    0],\n",
       "        [ 652,    0],\n",
       "        [ 653,    0],\n",
       "        [ 668,    0],\n",
       "        [ 678,    0],\n",
       "        [ 679,    0],\n",
       "        [ 685,    0],\n",
       "        [ 688,    0],\n",
       "        [ 695,    0],\n",
       "        [ 705,    0],\n",
       "        [ 721,    0],\n",
       "        [ 724,    0],\n",
       "        [ 738,    0],\n",
       "        [ 760,    0],\n",
       "        [ 767,    0],\n",
       "        [ 771,    0],\n",
       "        [ 785,    0],\n",
       "        [ 823,    0],\n",
       "        [ 834,    0],\n",
       "        [ 838,    0],\n",
       "        [ 845,    0],\n",
       "        [ 864,    0],\n",
       "        [ 875,    0],\n",
       "        [ 888,    0],\n",
       "        [ 890,    0],\n",
       "        [ 900,    0],\n",
       "        [ 909,    0],\n",
       "        [ 917,    0],\n",
       "        [ 921,    0],\n",
       "        [ 922,    0],\n",
       "        [ 937,    0],\n",
       "        [ 957,    0],\n",
       "        [ 964,    0],\n",
       "        [ 968,    0],\n",
       "        [ 980,    0],\n",
       "        [ 993,    0],\n",
       "        [1003,    0],\n",
       "        [1013,    0],\n",
       "        [1030,    0],\n",
       "        [1035,    0],\n",
       "        [1036,    0],\n",
       "        [1055,    0],\n",
       "        [1064,    0],\n",
       "        [1073,    0],\n",
       "        [1087,    0],\n",
       "        [1091,    0],\n",
       "        [1099,    0],\n",
       "        [1120,    0],\n",
       "        [1130,    0],\n",
       "        [1134,    0],\n",
       "        [1149,    0],\n",
       "        [1163,    0],\n",
       "        [1172,    0],\n",
       "        [1183,    0],\n",
       "        [1192,    0],\n",
       "        [1209,    0],\n",
       "        [1214,    0],\n",
       "        [1220,    0],\n",
       "        [1291,    0],\n",
       "        [1331,    0],\n",
       "        [1337,    0],\n",
       "        [1350,    0],\n",
       "        [1351,    0],\n",
       "        [1369,    0],\n",
       "        [1383,    0],\n",
       "        [1393,    0],\n",
       "        [1397,    0],\n",
       "        [1406,    0],\n",
       "        [1407,    0],\n",
       "        [1408,    0],\n",
       "        [1437,    0],\n",
       "        [1452,    0],\n",
       "        [1470,    0],\n",
       "        [1532,    0],\n",
       "        [1546,    0],\n",
       "        [1551,    0],\n",
       "        [1564,    0],\n",
       "        [1588,    0],\n",
       "        [1610,    0],\n",
       "        [1623,    0],\n",
       "        [1638,    0],\n",
       "        [1641,    0],\n",
       "        [1655,    0],\n",
       "        [1662,    0],\n",
       "        [1671,    0],\n",
       "        [1696,    0],\n",
       "        [1697,    0],\n",
       "        [1749,    0],\n",
       "        [1755,    0],\n",
       "        [1759,    0],\n",
       "        [1768,    0],\n",
       "        [1776,    0],\n",
       "        [1801,    0],\n",
       "        [1842,    0],\n",
       "        [1858,    0],\n",
       "        [1872,    0],\n",
       "        [1880,    0],\n",
       "        [1887,    0],\n",
       "        [1898,    0],\n",
       "        [1928,    0],\n",
       "        [1933,    0],\n",
       "        [1934,    0],\n",
       "        [1952,    0],\n",
       "        [1965,    0],\n",
       "        [1983,    0],\n",
       "        [1989,    0],\n",
       "        [2010,    0],\n",
       "        [2013,    0],\n",
       "        [2019,    0],\n",
       "        [2022,    0],\n",
       "        [2027,    0],\n",
       "        [2053,    0],\n",
       "        [2063,    0],\n",
       "        [2065,    0],\n",
       "        [2069,    0],\n",
       "        [2073,    0],\n",
       "        [2085,    0],\n",
       "        [2105,    0],\n",
       "        [2106,    0],\n",
       "        [2118,    0],\n",
       "        [2122,    0],\n",
       "        [2151,    0],\n",
       "        [2159,    0],\n",
       "        [2174,    0],\n",
       "        [2176,    0],\n",
       "        [2209,    0],\n",
       "        [2216,    0],\n",
       "        [2232,    0],\n",
       "        [2243,    0],\n",
       "        [2256,    0],\n",
       "        [2273,    0],\n",
       "        [2291,    0],\n",
       "        [2330,    0],\n",
       "        [2353,    0],\n",
       "        [2364,    0],\n",
       "        [2408,    0],\n",
       "        [2415,    0],\n",
       "        [2463,    0],\n",
       "        [2464,    0],\n",
       "        [2510,    0],\n",
       "        [2543,    0],\n",
       "        [2558,    0],\n",
       "        [2588,    0],\n",
       "        [2591,    0],\n",
       "        [2605,    0],\n",
       "        [2626,    0],\n",
       "        [2646,    0],\n",
       "        [2652,    0],\n",
       "        [2654,    0],\n",
       "        [2709,    0],\n",
       "        [2729,    0],\n",
       "        [2734,    0],\n",
       "        [2755,    0],\n",
       "        [2760,    0],\n",
       "        [2761,    0],\n",
       "        [2764,    0],\n",
       "        [2772,    0],\n",
       "        [2809,    0],\n",
       "        [2811,    0],\n",
       "        [2820,    0],\n",
       "        [2827,    0],\n",
       "        [2835,    0],\n",
       "        [2848,    0],\n",
       "        [2853,    0],\n",
       "        [2878,    0],\n",
       "        [2880,    0],\n",
       "        [2896,    0],\n",
       "        [2916,    0],\n",
       "        [2926,    0],\n",
       "        [2937,    0],\n",
       "        [2943,    0],\n",
       "        [2965,    0],\n",
       "        [2994,    0],\n",
       "        [3081,    0],\n",
       "        [3101,    0],\n",
       "        [3111,    0],\n",
       "        [3115,    0],\n",
       "        [3130,    0],\n",
       "        [3155,    0],\n",
       "        [3172,    0],\n",
       "        [3178,    0],\n",
       "        [3200,    0],\n",
       "        [3201,    0],\n",
       "        [3206,    0],\n",
       "        [3212,    0],\n",
       "        [3219,    0],\n",
       "        [3304,    0],\n",
       "        [3326,    0],\n",
       "        [3327,    0],\n",
       "        [3356,    0],\n",
       "        [3364,    0],\n",
       "        [3379,    0],\n",
       "        [3382,    0],\n",
       "        [3394,    0],\n",
       "        [3418,    0],\n",
       "        [3422,    0],\n",
       "        [3425,    0],\n",
       "        [3447,    0],\n",
       "        [3449,    0],\n",
       "        [3455,    0],\n",
       "        [3462,    0],\n",
       "        [3483,    0],\n",
       "        [3492,    0],\n",
       "        [3503,    0],\n",
       "        [3518,    0],\n",
       "        [3540,    0],\n",
       "        [3552,    0],\n",
       "        [3591,    0],\n",
       "        [3606,    0],\n",
       "        [3647,    0],\n",
       "        [3653,    0],\n",
       "        [3658,    0],\n",
       "        [3659,    0],\n",
       "        [3666,    0],\n",
       "        [3671,    0],\n",
       "        [3675,    0],\n",
       "        [3700,    0],\n",
       "        [3746,    0],\n",
       "        [3750,    0],\n",
       "        [3758,    0],\n",
       "        [3759,    0],\n",
       "        [3800,    0],\n",
       "        [3805,    0],\n",
       "        [3808,    0],\n",
       "        [3809,    0],\n",
       "        [3811,    0],\n",
       "        [3826,    0],\n",
       "        [3860,    0],\n",
       "        [3873,    0],\n",
       "        [3882,    0],\n",
       "        [3892,    0],\n",
       "        [3895,    0],\n",
       "        [3906,    0],\n",
       "        [3920,    0],\n",
       "        [3940,    0],\n",
       "        [3956,    0],\n",
       "        [3958,    0],\n",
       "        [3968,    0],\n",
       "        [3990,    0]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.nonzero()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
